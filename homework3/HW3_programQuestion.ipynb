{
 "cells": [
  {
   "source": [
    "# HW3_programQuestion\n",
    "\n",
    "**Due to 11:59 pm, 28th, October 2020**\n",
    "\n",
    "**This is an individual assignment.**"
   ],
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TyAsw2JuVfaM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nYCgbkJwVg7r"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import libraries that you might require.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2aeHsfeSqlUD"
   },
   "source": [
    "## Logistic Regression and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4idcCOhWrWU0"
   },
   "source": [
    "In this question, we will try to use logistic regression to solve a binary classification problem. Given some information of the house, such as area and the number of living rooms, would it be expensive? We would like to predict 1 if it is expensive, and 0 otherwise. \n",
    "\n",
    "We will first implement it with a python package, and then try to implement it by updating weights with gradient descent.Batch gradient descent (since we are using all samples at each iteration) and AdaGrad will be implemented. We will also derive the gradient formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3gR3WFlfqlUS"
   },
   "source": [
    "### a) Implement logistic regression with Scikit learn package. \n",
    "\n",
    "First load data and observe data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VUMEYqDrqlUT"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(800, 11)\n(400, 11)\n(800, 1)\n(400, 1)\n   LotArea  OverallQual  OverallCond  YearBuilt  YearRemodAdd  GrLivArea  \\\n0    11851            7            5       1990          1990       1442   \n1     9920            7            5       1996          1997       2013   \n2    11216            8            5       2006          2006       1489   \n3     8244            7            5       2004          2004       1720   \n4    11787            7            5       1996          1997       2398   \n\n   FullBath  BedroomAbvGr  KitchenAbvGr  YrSold  const  \n0         2             3             1    2009      1  \n1         2             3             1    2007      1  \n2         2             3             1    2006      1  \n3         2             3             1    2007      1  \n4         2             3             1    2007      1  \n   label\n0      1\n1      1\n2      1\n3      1\n4      1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reads the data.\n",
    "\"\"\"\n",
    "\n",
    "X_train = pd.read_csv('hw3_house_sales/X_train.csv')\n",
    "X_test = pd.read_csv('hw3_house_sales/X_test.csv')\n",
    "y_train = pd.read_csv('hw3_house_sales/y_train.csv')\n",
    "y_test = pd.read_csv('hw3_house_sales/y_test.csv')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(X_test.head(5))\n",
    "print(y_test.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVmVoQfAUkqe"
   },
   "source": [
    "Fill in the logisticRegressionScikit() function. Report the weights, training accuracy, and the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "psZtnphuGJSj"
   },
   "outputs": [],
   "source": [
    "def LogisticRegressionScikit(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Computes logistic regression with scikit-learn.\n",
    "    \n",
    "    Args:\n",
    "        X_train: feature matrix of training set\n",
    "        y_train: truth value of training set\n",
    "        X_test: feature matrix of test set\n",
    "        y_test: truth value of test set\n",
    "\n",
    "    Returns:  \n",
    "        w: numpy array of learned coefficients\n",
    "        y_pred: numpy array of predicted labels for the test data\n",
    "        score: accuracy of test data\n",
    "    \"\"\"\n",
    "    clf = LogisticRegression(random_state=0).fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = clf.score(X_test,y_test)\n",
    "    coef = clf.coef_\n",
    "    return coef, y_pred, score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6GJ7cAepGwqp"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.95\nlogistic regression coefficient: [[ 3.30147860e-04  1.91758109e+00  1.01659015e+00  7.61657425e-02\n   1.36741708e-03  7.05591471e-03  2.48682303e-01 -9.27372661e-01\n  -4.52099059e-01 -9.03354524e-02 -2.16364255e-04]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "coef_scikit, y_pred_scikit, acc_scikit = LogisticRegressionScikit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(acc_scikit))\n",
    "print('logistic regression coefficient:', coef_scikit)"
   ]
  },
  {
   "source": [
    "### b) Gradient derivation\n",
    "Calculate the maximum likelihood estimation $L(w)=P(Y|X;w)$, then formulate the stochastic gradient ascent rule.\n",
    "\n",
    "$h(X;w) = \\frac{1}{1+\\exp^{-w^TX}}$\n",
    "\n",
    "$P(Y|X;w)=h(x;w)^Y(1-h(x;w))^{(1-y)}$\n",
    "\n",
    "$L(w) = \\prod \\limits_{i=1}^n p(Y^i|X^i;w)^{Y^i}(1-h(X^i;w))^{1-Y^i}$\n",
    "\n",
    "$l(w) = \\ln L(w) = \\sum \\limits_{i= 1}^{n}Y^i \\ln (h(X^i;w)) + (1-Y^i)\\ln(1-h(X^i;w))$\n",
    "\n",
    "$w_j := w_j + \\Delta w_j = w_j - \\eta \\frac{\\partial}{\\partial w_j} l(w)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial w_j}l(w) = \\sum \\limits_{i=1}^n(Y^i - h(X;w))x_j$\n",
    "\n",
    "$w_{j+1} = w_j - \\eta \\sum \\limits_{i = 1}^n(Y^i-h(X;w))x_j$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8I06kejCVxv8"
   },
   "source": [
    "### c) Logistic regression with simple gradient descent\n",
    "Fill in the LogisticRegressionSGD() function. To do that, two helper functions, sigmoid_activation() (to calculate the sigmoid function result), and model_optimize() (to calculate the gradient of w), will be needed. Both helper functions can be used in the following AdaGrad optimization function. Use a learning rate of $10^{−4}$, run with 2000 iterations. Report the weights and accuracy. Keep track of the accuracy every 100 iterations in the training set. It will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PpkEe4mkQ19p"
   },
   "outputs": [],
   "source": [
    "def sigmoid_activation(x):\n",
    "    \"\"\"\n",
    "    Calculates the sigmoid function.\n",
    "    \n",
    "    Args:\n",
    "        x: numpy array of input\n",
    "        \n",
    "    Returns:\n",
    "        final_result: numpy array of sigmoid result\n",
    "    \"\"\"\n",
    "    final_result = 1.0/(1+np.exp(-x))\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9BJvPP71bFs5"
   },
   "source": [
    "**Remember to derive the gradient (Question 4.2), write down the weight update formula, and hand it in with your latex submission!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2RGSw2AyQ_kE"
   },
   "outputs": [],
   "source": [
    "def model_optimize(w, X, Y):\n",
    "    \"\"\"\n",
    "    Calculates gradient of the weights.\n",
    "    \n",
    "    Args:\n",
    "        X: numpy array of training samples\n",
    "        Y: numpy array of training labels\n",
    "        w: numpy array of weights\n",
    "    Returns:\n",
    "        dw: the gradient of the weights\n",
    "\n",
    "    \"\"\"\n",
    "    h = sigmoid_activation(X*w)\n",
    "    error = h - Y\n",
    "    dw = X.T * error\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VI5Fdz6ZRUQ9"
   },
   "outputs": [],
   "source": [
    "def LogisticRegressionSGD(w, X, Y, learning_rate, num_iterations):\n",
    "    \"\"\"\n",
    "    Uses SGD to update weights for logistic regression.\n",
    "    \n",
    "    Args:       \n",
    "        w: numpy array of initial weights\n",
    "        X: numpy array of training samples\n",
    "        Y: numpy array of training labels\n",
    "        learning_rate: float learning rate to update w\n",
    "        num_iterations: int number of iterations to update w\n",
    "    \n",
    "    Returns:  \n",
    "        coeff: numpy array of weights after optimization\n",
    "        accuracies: a list of accuracy at each hundred's iteration. With 2000 iterations, \n",
    "                    accuracies should be a list of size 20 \n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    accuracies = []\n",
    "    for i in range(n):\n",
    "        dw = model_optimize(w,X[i],Y[i])\n",
    "        w = w - learning_rate * dw\n",
    "        if i % 100 == 0:\n",
    "            print(w)\n",
    "            print(X)\n",
    "            Y_test = np.dot(w,X.T)\n",
    "            print(Y_test)\n",
    "            Error = Y_test - Y\n",
    "            for i in range(len(Error)):\n",
    "                if Error[i] == Y_test[i]:\n",
    "                    accur += 1\n",
    "            accur = accur / len(Error)\n",
    "            accuracies.append(accur)\n",
    "    test_Y = w*X\n",
    "    coeff = w\n",
    "    return coeff, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AQ2M5WETZ4f0"
   },
   "source": [
    "### d) Logistic regression with AdaGrad\n",
    " Fill in the LogisticRegressionAda() function. Use a learning rate of $10^{−4}$, run with 2000 iterations. Report the weights and accuracy. Keep track of the accuracy every 100 iterations in the training set. It will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cIO8iOzJRscr"
   },
   "outputs": [],
   "source": [
    "def LogisticRegressionAda(w, X, Y, learning_rate, num_iterations):\n",
    "    \"\"\"\n",
    "    Use AdaGrad to update weights.\n",
    "    \n",
    "    Args:       \n",
    "        w: numpy array of initial weights\n",
    "        X: numpy array of training samples\n",
    "        Y: numpy array of training labels\n",
    "        learning_rate: float learning rate to update w\n",
    "        num_iterations: int number of iterations to update w\n",
    "    \n",
    "    Returns:   \n",
    "        coeff: numpy array of weights after optimization\n",
    "        accuracies: a list of accuracy at each hundred's iteration\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "\n",
    "    return coeff, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TScnzScyaCZM"
   },
   "source": [
    "We add a predict() function here to threshold probability prediction into binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ypERS0E1SLul"
   },
   "outputs": [],
   "source": [
    "def predict(final_pred, m):\n",
    "    \"\"\"\n",
    "    Predict labels from probability to 0/1 label, threshold 0.5.\n",
    "    \n",
    "    Args:\n",
    "        final_pred: m x 1 vector, probabilty of each sample belonging to class 1\n",
    "        m: number of samples\n",
    "        \n",
    "    Returns:\n",
    "        y_pred: m x 1 vector, label of each sample, can be 0/1\n",
    "    \"\"\"\n",
    "    y_pred = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        if final_pred < 0.5:\n",
    "            y_pred[i] = 0\n",
    "        else:\n",
    "            y_pred[i] = 1\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iLQA1SHAaRQn"
   },
   "source": [
    "Now we start to use our dataset and construct model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HHtOjixmSh86"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Do some data preparation, convert dataframe to numpy array\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "w = np.zeros((1, n_features))\n",
    "\n",
    "# X_train = X_train.values\n",
    "# X_test = X_test.values\n",
    "\n",
    "# y_train = y_train.values\n",
    "# y_test = y_test.values\n",
    "\n",
    "m_train =  X_train.shape[0]\n",
    "m_test =  X_test.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S-aVaiJzqlU0"
   },
   "source": [
    "Model construction for SGD logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tpa3o6VMTdTc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-1.261e-01 -3.500e-04 -2.500e-04 -1.002e-01 -1.002e-01 -8.545e-02\n  -1.000e-04 -1.500e-04 -5.000e-05 -1.003e-01 -5.000e-05]]\n[[ 2522     7     5 ...     1  2006     1]\n [ 6000     6     6 ...     1  2006     1]\n [12735     4     5 ...     1  2008     1]\n ...\n [10880     5     5 ...     1  2008     1]\n [11362     8     5 ...     1  2009     1]\n [ 5000     1     3 ...     1  2007     1]]\n[[ -1066.8661   -1443.33825  -2276.3068   -1376.3494   -2116.04355\n   -1884.16335  -1581.0073   -1585.9988   -1670.5768   -1761.51795\n   -2029.43935  -2125.12905  -2018.5155    -939.1024   -1960.3272\n   -2356.62235  -2362.9867   -2601.8427   -1451.10895  -2146.744\n   -2144.2417   -1851.86815  -2182.19145  -2076.77555  -1586.4923\n   -1999.82585  -3403.81525  -1662.0472   -1997.7992   -1935.57675\n   -1261.82945  -2060.55205  -1731.3898   -2219.36355  -2359.67285\n   -2788.583    -2084.35715  -1744.78625  -1831.71185  -2091.92355\n   -1777.11185  -2206.5719   -1989.8087   -2075.3952   -5420.3689\n   -2221.18235  -2859.8607   -1991.7486   -1785.2905   -2396.7664\n   -2609.5921   -1893.77135  -2284.9738   -2016.0267   -1957.33655\n   -1807.7163   -2442.83505  -1748.22195  -1782.4994   -2435.4015\n   -1912.43385  -1701.87595  -1815.4292   -1800.49275  -1749.78865\n   -2074.28865  -1864.7415   -1178.9593   -2466.42075  -2719.30675\n   -2697.658    -2292.11395  -1789.35705  -1875.49755  -1431.1873\n   -2391.76015  -1706.12615  -1773.5671   -2445.5816   -1976.51425\n   -2103.67495  -1447.9447   -1821.8437   -1865.80605  -2329.68895\n   -1776.5477   -1009.9215   -2439.29415  -1654.14465  -1590.0879\n   -1950.88905  -1937.2472   -1910.87435 -21511.4321   -2344.72825\n   -1172.83075  -1198.76575  -1587.7493   -1584.43085  -1575.82215\n    -888.7572   -2121.79455  -1969.7735   -2071.00385  -2069.6608\n   -1902.07415  -1875.73955  -3481.1023   -2532.25885   -896.39025\n   -1865.5961   -2278.48565  -1931.70145  -1806.3752   -1453.2701\n   -1824.0745   -2687.29385  -2166.842    -1994.4735   -1631.2796\n   -1951.5342   -2176.61205  -1858.88465  -1849.1082   -1554.4913\n   -1801.5757   -1683.83015  -2027.48955  -2275.5985   -1387.6284\n   -2331.191    -1624.08045  -2782.75255  -1466.2781   -1234.5224\n   -2449.08245  -1137.53415  -1851.5126   -1158.0272   -2265.245\n   -1720.80465  -1821.19     -2173.04895   -836.37665  -2627.9745\n   -2506.125    -1886.9811   -1656.145    -2497.38805   -916.21455\n   -1907.30425  -1772.18165  -1957.0327   -2576.9443   -1783.0238\n   -1952.10405  -1234.7229   -2792.19445  -1926.213    -1509.7141\n   -3433.81895  -1984.45695  -2087.7818   -1985.78865  -1624.4886\n   -1909.82635  -1200.378    -1568.9108   -1915.69525  -2065.3619\n   -2893.1742   -1106.5461   -2596.8065   -1448.4581   -1388.24425\n   -1960.02425  -2022.63025  -2370.8371   -1329.0658   -1501.43735\n   -2091.9393   -2508.04245  -1290.3004   -2184.4603   -2120.03525\n   -1572.2152   -2936.87425   -961.56735  -1907.95545  -1882.13925\n   -1233.5879   -2771.68365  -1716.6796   -1687.7996   -1893.7266\n   -2191.0027   -1791.97445  -2884.6482   -2102.3342   -3257.0523\n   -1957.88275  -1790.44785  -1893.79425  -1409.32075  -1502.69085\n   -1708.96555  -1710.40025  -2186.55415  -1597.0294   -2501.48465\n   -1767.7993   -3872.1232   -1419.14595  -2652.3901   -2754.28865\n   -2029.0062   -2032.6928   -1855.1654   -2364.00075  -3296.2774\n   -2143.72235  -2197.0275   -1220.52005  -2162.88785  -1687.684\n   -2618.89045  -1963.00695  -2480.24315  -1830.48685  -2323.58715\n   -1454.78275  -1938.35365  -3685.3716   -1337.35215  -1739.87635\n   -2354.6624   -1830.4278   -1395.3604   -2004.9148   -1674.16145\n   -1905.60585  -1500.0452   -1907.218    -1664.8979   -1866.66355\n   -2574.4021   -1483.24955  -2257.88065  -1702.10885  -1718.02505\n   -2085.80245  -2291.5423   -2010.26935  -1340.5232   -1589.13905\n   -1887.25715  -2313.676    -2033.33085  -1934.9336   -2033.6107\n   -3492.90625  -1768.30385  -2116.29625  -1877.70115  -2953.84075\n   -2871.68455  -1708.0356   -2216.91365  -3792.78405  -1656.3013\n   -1662.32475  -2024.73585  -1111.6351   -2049.5792   -1960.26615\n   -1717.51965  -1800.54975  -1668.30255  -1932.77925  -2073.5467\n   -2405.027    -1577.69555  -2418.2283   -1281.06665  -1594.40985\n   -1791.5264   -1179.0791   -2834.5671   -2532.6496   -1158.12785\n   -1804.0224   -2424.75565  -1660.66905  -1589.65765  -1965.29945\n   -1814.92745  -2424.26955  -2074.816    -1898.50555  -1829.34585\n   -2023.7518   -2749.05495  -1980.52965  -2332.1221   -1636.8012\n   -2131.0189   -1217.7739   -1680.86115  -1130.03845  -2049.245\n   -1494.12335  -2121.69735  -1558.1815   -2143.6001   -1777.6382\n   -1899.43375  -2857.42845  -2380.4742   -1932.89575  -2191.92215\n   -1794.7856    -867.24685  -1354.5356   -2316.7865   -1228.1184\n   -1661.0307   -2254.32905  -2052.89175  -9650.95305  -2059.3798\n   -2352.4876   -2243.05455  -2155.73815  -1859.0104   -1861.4294\n   -2025.62545  -2012.97215  -1878.21525  -2054.2615   -1966.6053\n   -1140.75125  -1968.17045   -912.1203   -1808.82535  -1143.0539\n   -1749.3896   -1795.9688   -2110.33245  -1908.60835  -2012.71235\n   -1729.39855  -1059.492    -2340.06485   -939.2283   -2882.63545\n   -1370.17435  -1943.05525  -1675.05085  -2160.15935  -3102.3746\n   -1874.6762   -2032.0082   -1475.0197   -2565.80075  -2162.9794\n   -2277.40825  -2576.47765  -1760.30115  -1468.4403    -892.5816\n   -1715.83335  -1457.15215  -1996.69155  -3910.83645  -1922.6571\n   -2490.93865  -1927.4518   -3384.75745  -1754.6692   -1855.3676\n   -3133.3759   -1898.2472   -1696.5742   -2022.30675  -1957.79895\n    -892.5813   -2372.3173   -1523.7353   -2245.6856   -2218.21635\n   -3857.63535  -2179.6867    -892.9826   -1950.5611   -2574.79555\n   -1451.59525  -1905.53265  -1831.8351   -1744.72605  -1740.94185\n   -2206.04205  -2173.74615  -1879.4865   -2271.2381   -1430.9737\n   -2249.6152   -1573.52955  -2534.61155  -5159.2894   -2274.71645\n   -2229.08085  -2057.4221   -1706.65285  -2274.4148   -1702.4791\n   -1698.1401   -2064.63865  -1493.8567   -1848.9751   -2090.4692\n   -1579.67395  -2047.41805  -1885.88255  -1996.3268   -1236.78535\n   -2394.88635  -2264.9379   -3385.441    -3640.263    -1765.311\n   -1963.29975  -1573.34755  -1931.5144   -1904.22135  -2305.3116\n   -1728.6289   -1752.16     -1300.57835  -1576.4751   -1693.42345\n   -1458.2993   -1575.44865  -1796.0729   -1851.49385  -2020.66495\n   -1802.37885  -2179.95415  -2757.4594   -1718.3248   -1115.8975\n   -2223.4609   -1752.07925  -1780.5803   -1893.60365  -1778.76125\n   -1876.0511   -1801.0885   -2354.1249   -1930.24675  -4116.19655\n   -2431.3783   -1865.09435  -1403.48725  -1796.26805  -1572.2088\n   -2522.4021   -2690.65325  -1746.90555  -2183.57015  -1918.5183\n   -1849.466    -2321.3928   -1800.85415  -2182.6532    -843.13845\n   -1889.75885  -2325.89755  -2086.74945  -1852.01675  -1416.00555\n   -7630.0542   -1492.2912   -2191.96055  -1699.80825  -2038.07315\n   -1509.35675  -1788.1784   -1234.4221   -1638.02155  -2028.0125\n   -1780.4271   -2237.14625  -1528.25525  -2154.39315  -2041.08985\n   -1577.18655  -2000.54375  -1763.8576   -1588.9738   -1158.22815\n   -1730.7426   -2228.46865  -2106.1752   -2184.0054   -2530.96135\n   -1944.8329   -1760.6196   -1723.49235  -1793.67395  -1808.36375\n   -2505.52075  -1175.27615  -1793.2452   -1937.7965   -2007.95395\n   -1314.89745  -1789.779    -1685.183    -2066.8802   -2019.16515\n   -4262.36195  -1758.1507   -1964.1285   -1886.1667   -1582.32235\n   -3420.0871   -1701.24695  -1582.9898   -1453.23285  -3369.34775\n   -2031.79085  -2419.64955  -2212.9501   -2639.70445  -3703.75395\n   -1241.458    -2338.12035  -1782.60745  -1792.6811   -2457.4745\n   -1971.16585  -2218.8579   -1853.58295  -1653.29325  -2309.72975\n   -1568.6856   -2648.44685  -2670.35155  -2272.2698   -1596.5444\n   -1970.70225  -2076.28835  -1575.76795  -2195.8435   -2191.6985\n   -2786.11445  -1986.66005  -1750.8921   -1795.77245  -1699.58655\n   -1962.0051   -1393.64715  -1671.82285   -901.7548   -2149.78985\n   -2099.3421   -1791.25965  -2819.0074   -1658.84655  -2084.6573\n   -2154.60575  -1066.8661   -1196.86275  -2172.31205  -2319.5283\n   -1215.18705  -2338.08805  -1885.12265  -1741.7395   -1716.9603\n   -2162.795    -2734.38525  -2152.78015  -2463.57905  -2405.889\n   -2831.8759   -1559.421    -1001.1001   -1976.9145   -1449.49435\n   -2056.84405  -2183.3046   -1359.4194   -1924.5633   -1754.26025\n   -1930.3327   -1905.70435  -3478.48205  -2101.64885  -1810.45125\n   -2630.68735   -927.2073   -3079.71715  -7095.3719   -2319.2675\n   -1280.0273   -1841.14645  -3412.566    -2771.57145  -2601.7533\n   -1703.67885  -1429.13985  -1946.91065  -1843.4128   -1559.5215\n   -2082.2357   -2234.32605  -2318.74065  -1701.23685   -892.5818\n   -2375.41515  -2147.32275  -2089.12935   -919.59875  -1751.7743\n   -1818.5899   -2136.86855  -1320.1307   -2249.0648   -2103.0894\n   -1720.41445  -1881.969    -2150.08745  -1694.0553  -20831.60445\n    -941.7309   -1765.1205   -2106.90975  -1622.9252   -2270.4774\n   -1111.0132   -1915.22315  -1743.43435  -2840.1716   -1993.00425\n   -2330.17865  -2141.5978   -1379.38175  -1718.7922   -1429.4867\n   -2437.5059   -1290.8715   -1341.2406   -6686.26245  -1895.23325\n   -1825.9062   -1899.7956   -2317.6628   -1514.92755   -912.3212\n   -2303.3977   -2888.5159   -2571.17585  -2182.4033   -2237.16905\n   -1770.44665  -1379.2191   -1807.78075  -2221.53355  -1477.73605\n   -2273.72945  -2619.63005  -2040.0883   -2386.63865  -1588.77295\n   -2021.0021   -1674.47635  -2521.42245  -3834.47145  -1644.34575\n   -2237.6789   -4064.0444   -1787.34235  -1431.27785  -1581.119\n   -2019.3534   -2854.96615  -1940.1572   -1852.38385  -1739.5982\n   -1854.4225   -1796.19085  -1842.34065  -1814.85185  -2280.5987\n   -1763.261    -2229.4447   -2923.901    -2375.9517   -1908.2771\n   -1781.0022   -1732.54835 -15275.5507   -2667.951    -2040.48565\n   -1467.4635   -1476.36665  -2051.13485  -2024.14495  -1961.43435\n   -1954.20325  -1941.5247   -1700.8128   -1685.9461   -2398.237\n   -1689.1542   -2206.3379   -2991.94655  -2073.27255  -2406.9353\n   -2174.92715  -1986.7023   -2024.21655  -2967.7763   -1955.3125\n   -1235.1011   -1809.3218   -2588.21245  -2735.73465  -1703.2257\n   -2064.22155  -1913.6709   -2047.24675  -2432.528    -1873.5174\n    -969.16885  -1993.31935  -1771.56995  -2573.72575  -2016.9379\n   -1398.3172   -2265.54355  -1575.1881   -1973.9096   -1579.0457\n   -1133.0759   -1867.5184   -1600.5702   -2462.0597   -1227.52645\n   -2155.17285  -2243.3464   -2193.21365  -2437.11775  -1913.4967\n   -1584.93045  -1871.25845  -2096.38005  -1792.02635  -1495.5093\n   -2887.85625  -2034.41385  -1159.5627   -2570.1287   -1685.11895\n   -2555.8413   -1439.5796   -2048.40095  -2123.71675  -1641.14455\n   -2117.1357   -2348.88545  -1532.71185  -2248.6265    -894.05645\n   -1994.8925   -1163.1425   -2571.919    -1762.44035  -1801.49445\n   -1553.704    -1112.9957   -2481.8042   -1028.31185  -1346.7174\n   -1503.3433   -1685.3617   -1862.7917   -2261.0108   -1792.72275\n   -2555.33245  -1944.2731   -7451.02345  -1418.13585  -1017.7749\n   -2163.41145  -1906.7892   -2063.61785  -2192.8437   -1250.72305]]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-36e53b200846>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Gradient Descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcoeff_SGD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_SGD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegressionSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_SGD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# TODO: predict probability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfinal_train_pred_SGD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-90-a0c526b03ec5>\u001b[0m in \u001b[0;36mLogisticRegressionSGD\u001b[1;34m(w, X, Y, learning_rate, num_iterations)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mError\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mError\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m                     \u001b[0maccur\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0maccur\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccur\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "\n",
    "#Gradient Descent\n",
    "coeff_SGD, acc_SGD = LogisticRegressionSGD(w, X_train, y_train, learning_rate=0.0001,num_iterations=2000)\n",
    "print(acc_SGD)\n",
    "# TODO: predict probability\n",
    "final_train_pred_SGD = ...\n",
    "final_test_pred_SGD = ...\n",
    "# predict label\n",
    "y_train_pred_SGD = predict(final_train_pred_SGD, m_train)\n",
    "y_test_pred_SGD = predict(final_test_pred_SGD, m_test)\n",
    "\n",
    "print('Optimized weights for SGD', coeff_SGD[:-1])\n",
    "print('Optimized intercept for SGD', coeff_SGD[-1])\n",
    "\n",
    "print('Training Accuracy for SGD', accuracy_score(y_train_pred_SGD.T, y_train))\n",
    "print('Test Accuracy for SGD', accuracy_score(y_test_pred_SGD.T, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4cSHZdx-amhq"
   },
   "source": [
    "Model construction for AdaGrad logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "idXxXj3zTcwp"
   },
   "outputs": [],
   "source": [
    "\n",
    "#AdaGrad Descent\n",
    "coeff_Ada, acc_Ada = LogisticRegressionSGD(w, X_train, y_train, learning_rate=0.0001,num_iterations=2000)\n",
    "\n",
    "# TODO: predict probability\n",
    "final_train_pred_Ada = ...\n",
    "final_test_pred_Ada = ...\n",
    "# predict label\n",
    "y_train_pred_Ada = predict(final_train_pred_Ada, m_train)\n",
    "y_test_pred_Ada = predict(final_test_pred_Ada, m_test)\n",
    "\n",
    "print('Optimized weights for Ada', coeff_Ada[:-1])\n",
    "print('Optimized intercept for Ada', coeff_Ada[-1])\n",
    "\n",
    "print('Training Accuracy for Ada', accuracy_score(y_train_pred_Ada.T, y_train))\n",
    "print('Test Accuracy for Ada', accuracy_score(y_test_pred_Ada.T, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "egjfJWHWasSQ"
   },
   "source": [
    "Plot accuracy vs iteration for SGD and AdaGrad. Compare the performance difference. Briefly explain the reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AFiMS7M4UEU7"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot accuracy vs iteration for SGD and AdaGrad\n",
    "\n",
    "plt.plot(acc_SGD, label='SGD')\n",
    "plt.plot(acc_Ada, label='AdaGrad')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title('Accuracy improvement over time')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Comparision of Scikit, SGD and AdaGrad convergence\n",
    "Plot the loss function of SGD and AdaGrad over 2000 iterations on both the training and test data. What do you observe? Which one has better accuracy on the test dataset? Why might that be the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}